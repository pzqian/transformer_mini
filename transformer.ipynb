{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4d93000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "07f14a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads=1, d_k=None, d_v=None):\n",
    "        super(ScaledProductAttention, self).__init__()\n",
    "        # make sure hidden_size is a tensor\n",
    "        if not torch.is_tensor(hidden_size):\n",
    "            hidden_size = torch.tensor(hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k if d_k is not None else hidden_size // num_heads\n",
    "        self.d_v = d_v if d_v is not None else hidden_size // num_heads\n",
    "        self.proj_K = torch.nn.Linear(hidden_size, num_heads * self.d_k)\n",
    "        self.proj_Q = torch.nn.Linear(hidden_size, num_heads * self.d_k)\n",
    "        self.proj_V = torch.nn.Linear(hidden_size, num_heads * self.d_v)\n",
    "        self.proj_0 = torch.nn.Linear(num_heads * self.d_v, hidden_size)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Each of Q, K, V is a tensor of shape (batch_size, seq_len, hidden_size). \n",
    "            NOTE: seq_len might be different for Q, K, V.\n",
    "        seq_len can be different for Q, K, V.\n",
    "        mask is of shape (batch_size, seq_len_q, seq_len_k).\n",
    "            NOTE: a lot of implementations are assuming seq_len_q and seq_len_k are the same. \n",
    "        For example, Q could be the length of the input. K and V could be the length of the output.\n",
    "        \"\"\"\n",
    "        proj_K = self.proj_K(K) # (batch_size, seq_len, num_heads * d_k)\n",
    "        proj_Q = self.proj_Q(Q) # (batch_size, seq_len, num_heads * d_k)\n",
    "        proj_V = self.proj_V(V) # (batch_size, seq_len, num_heads * d_v)\n",
    "        scores = torch.matmul(proj_Q, proj_K.transpose(1, 2)) # (batch_size, seq_len_q, seq_len_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        sm = torch.softmax(scores / torch.sqrt(self.d_k), dim=-1)\n",
    "        multihead = torch.matmul(sm, proj_V) # (batch_size, seq_len, num_heads * d_v)\n",
    "        return self.proj_0(multihead) # (batch_size, seq_len, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "72d71341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_K.weight torch.Size([10, 10])\n",
      "proj_K.bias torch.Size([10])\n",
      "proj_Q.weight torch.Size([10, 10])\n",
      "proj_Q.bias torch.Size([10])\n",
      "proj_V.weight torch.Size([10, 10])\n",
      "proj_V.bias torch.Size([10])\n",
      "proj_0.weight torch.Size([10, 10])\n",
      "proj_0.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "att = ScaledProductAttention(10, 2)\n",
    "for name, parameter in att.named_parameters():\n",
    "    print(name, parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "383a80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5710,  0.0048, -0.2483],\n",
      "         [ 0.5708,  0.0047, -0.2484]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding matrix (2 tokens x 3 dimensions)\n",
    "E = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "E = E.unsqueeze(0)\n",
    "\n",
    "# Q, K, V are all the same in masked self-attention\n",
    "Q = K = V = E\n",
    "\n",
    "attention = ScaledProductAttention(3)\n",
    "print(attention(Q, K, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b2261690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 512])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "n = 5 # input sequence length.\n",
    "m = 7 # output sequence length.\n",
    "hidden_size = 512 # hidden size of the model.\n",
    "Q = torch.randn(batch_size, n, hidden_size)\n",
    "K = torch.randn(batch_size, m, hidden_size)\n",
    "V = torch.randn(batch_size, m, hidden_size)\n",
    "\n",
    "attention = ScaledProductAttention(hidden_size)\n",
    "attention(Q, K, V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e10dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_K.weight torch.Size([512, 512])\n",
      "proj_K.bias torch.Size([512])\n",
      "proj_Q.weight torch.Size([512, 512])\n",
      "proj_Q.bias torch.Size([512])\n",
      "proj_V.weight torch.Size([512, 512])\n",
      "proj_V.bias torch.Size([512])\n",
      "proj_0.weight torch.Size([512, 512])\n",
      "proj_0.bias torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# print all the parameters of attention with their variable names.\n",
    "for name, param in attention.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1f247423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('proj_K.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0063,  0.0320, -0.0258,  ...,  0.0152, -0.0292, -0.0202],\n",
       "          [ 0.0054, -0.0347,  0.0073,  ...,  0.0214, -0.0263,  0.0418],\n",
       "          [ 0.0061,  0.0122,  0.0023,  ..., -0.0016, -0.0086,  0.0257],\n",
       "          ...,\n",
       "          [-0.0388,  0.0429, -0.0303,  ..., -0.0210, -0.0130,  0.0192],\n",
       "          [ 0.0114, -0.0317, -0.0416,  ...,  0.0356,  0.0072,  0.0346],\n",
       "          [-0.0333, -0.0213, -0.0220,  ...,  0.0092,  0.0182,  0.0368]],\n",
       "         requires_grad=True)),\n",
       " ('proj_K.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0140, -0.0353, -0.0335,  0.0274, -0.0144,  0.0137,  0.0040, -0.0132,\n",
       "          -0.0076, -0.0323,  0.0168, -0.0005,  0.0066,  0.0056, -0.0164,  0.0148,\n",
       "           0.0163,  0.0426, -0.0388,  0.0269,  0.0191, -0.0119, -0.0343, -0.0105,\n",
       "          -0.0265,  0.0134, -0.0362,  0.0318, -0.0219, -0.0128,  0.0380, -0.0307,\n",
       "           0.0310, -0.0017,  0.0017, -0.0106, -0.0273,  0.0083,  0.0058, -0.0023,\n",
       "           0.0072, -0.0342,  0.0307,  0.0259,  0.0080, -0.0417, -0.0137, -0.0298,\n",
       "          -0.0329, -0.0405,  0.0187,  0.0423, -0.0079,  0.0084, -0.0120, -0.0166,\n",
       "           0.0019,  0.0078,  0.0056,  0.0157,  0.0026, -0.0073,  0.0222,  0.0119,\n",
       "          -0.0299, -0.0025,  0.0170,  0.0281, -0.0270,  0.0079,  0.0072, -0.0240,\n",
       "           0.0149, -0.0297,  0.0344, -0.0439, -0.0393,  0.0417,  0.0036, -0.0005,\n",
       "          -0.0438, -0.0266, -0.0133,  0.0210,  0.0214, -0.0123,  0.0227,  0.0178,\n",
       "           0.0374,  0.0144, -0.0020, -0.0256,  0.0170, -0.0247, -0.0290, -0.0397,\n",
       "          -0.0008, -0.0422, -0.0301, -0.0241,  0.0106,  0.0280, -0.0172,  0.0142,\n",
       "          -0.0079,  0.0210,  0.0042, -0.0205,  0.0166,  0.0318, -0.0171,  0.0136,\n",
       "           0.0389,  0.0319, -0.0386, -0.0161, -0.0212,  0.0412,  0.0090,  0.0261,\n",
       "          -0.0435, -0.0163,  0.0149, -0.0268, -0.0142, -0.0349, -0.0326,  0.0052,\n",
       "          -0.0038, -0.0056, -0.0174,  0.0266, -0.0047,  0.0251, -0.0272,  0.0260,\n",
       "          -0.0209,  0.0429, -0.0064, -0.0178,  0.0008, -0.0033, -0.0350, -0.0024,\n",
       "           0.0306, -0.0321, -0.0231,  0.0217,  0.0221,  0.0058,  0.0324,  0.0103,\n",
       "          -0.0350,  0.0440,  0.0171, -0.0303, -0.0094, -0.0127,  0.0108,  0.0077,\n",
       "          -0.0038,  0.0087, -0.0071, -0.0313,  0.0401,  0.0258, -0.0356, -0.0319,\n",
       "          -0.0363,  0.0139,  0.0041,  0.0369,  0.0204, -0.0038,  0.0236,  0.0131,\n",
       "          -0.0206, -0.0041,  0.0176, -0.0419, -0.0045, -0.0071, -0.0413, -0.0078,\n",
       "          -0.0139, -0.0061,  0.0258,  0.0026,  0.0272, -0.0432, -0.0147, -0.0314,\n",
       "           0.0300,  0.0161,  0.0049, -0.0154, -0.0196,  0.0095,  0.0320, -0.0265,\n",
       "           0.0082, -0.0360, -0.0168, -0.0079,  0.0245,  0.0272,  0.0303, -0.0426,\n",
       "          -0.0303, -0.0291, -0.0025, -0.0242, -0.0138, -0.0042, -0.0110,  0.0345,\n",
       "           0.0340,  0.0292,  0.0019,  0.0274,  0.0040, -0.0062,  0.0357, -0.0336,\n",
       "           0.0234, -0.0319, -0.0283,  0.0012,  0.0064,  0.0326,  0.0378, -0.0183,\n",
       "           0.0390,  0.0104,  0.0088, -0.0246, -0.0260, -0.0023, -0.0040, -0.0316,\n",
       "           0.0397,  0.0337,  0.0335, -0.0077, -0.0147, -0.0214, -0.0412,  0.0401,\n",
       "           0.0020, -0.0014, -0.0149, -0.0120,  0.0058,  0.0071,  0.0312,  0.0232,\n",
       "          -0.0184,  0.0434,  0.0319,  0.0359,  0.0080,  0.0356,  0.0202,  0.0061,\n",
       "           0.0365,  0.0205,  0.0139, -0.0388,  0.0324,  0.0304, -0.0084,  0.0404,\n",
       "          -0.0158,  0.0090,  0.0122, -0.0111,  0.0360,  0.0357,  0.0030, -0.0286,\n",
       "           0.0166,  0.0372,  0.0215,  0.0022, -0.0399,  0.0394,  0.0028,  0.0373,\n",
       "          -0.0210,  0.0234, -0.0026,  0.0067, -0.0201,  0.0013,  0.0328,  0.0091,\n",
       "           0.0125, -0.0329, -0.0067,  0.0343, -0.0391,  0.0199,  0.0280, -0.0058,\n",
       "          -0.0417, -0.0099,  0.0106, -0.0053, -0.0155,  0.0378, -0.0060, -0.0056,\n",
       "          -0.0122, -0.0341,  0.0371,  0.0441,  0.0096,  0.0015,  0.0190,  0.0041,\n",
       "          -0.0416, -0.0188, -0.0296, -0.0411,  0.0271,  0.0422,  0.0318,  0.0117,\n",
       "           0.0412, -0.0318,  0.0254, -0.0274, -0.0002, -0.0226, -0.0159, -0.0433,\n",
       "           0.0272, -0.0267,  0.0409, -0.0164,  0.0055, -0.0119,  0.0026, -0.0340,\n",
       "          -0.0144, -0.0023,  0.0119,  0.0419,  0.0271, -0.0221,  0.0327,  0.0196,\n",
       "           0.0333,  0.0204,  0.0091,  0.0313,  0.0349, -0.0315, -0.0155,  0.0279,\n",
       "           0.0075, -0.0115,  0.0233,  0.0312, -0.0161, -0.0175, -0.0345,  0.0149,\n",
       "          -0.0277,  0.0378,  0.0049,  0.0389,  0.0048, -0.0276,  0.0392, -0.0052,\n",
       "           0.0248,  0.0135,  0.0220, -0.0294, -0.0258, -0.0045, -0.0052, -0.0098,\n",
       "          -0.0062, -0.0435, -0.0231,  0.0242,  0.0224, -0.0299,  0.0214,  0.0299,\n",
       "           0.0108, -0.0189,  0.0074,  0.0427,  0.0385, -0.0301, -0.0303, -0.0031,\n",
       "           0.0341,  0.0377,  0.0237, -0.0024, -0.0373,  0.0141, -0.0049, -0.0350,\n",
       "          -0.0366,  0.0210,  0.0291,  0.0274,  0.0281,  0.0250, -0.0431,  0.0337,\n",
       "           0.0055,  0.0174,  0.0192,  0.0141, -0.0228,  0.0217, -0.0123,  0.0135,\n",
       "           0.0105, -0.0368,  0.0435, -0.0346, -0.0437, -0.0075, -0.0280, -0.0266,\n",
       "           0.0026, -0.0066, -0.0355, -0.0409,  0.0021,  0.0028,  0.0354,  0.0014,\n",
       "           0.0103, -0.0016, -0.0148,  0.0410,  0.0303, -0.0325, -0.0123,  0.0313,\n",
       "          -0.0118, -0.0406,  0.0004,  0.0062, -0.0237,  0.0057, -0.0349,  0.0013,\n",
       "           0.0185,  0.0192,  0.0037,  0.0405,  0.0245, -0.0349, -0.0171, -0.0102,\n",
       "          -0.0041,  0.0141, -0.0053, -0.0021,  0.0236, -0.0230,  0.0109, -0.0065,\n",
       "          -0.0085,  0.0195,  0.0041, -0.0381, -0.0105, -0.0374, -0.0277, -0.0133,\n",
       "           0.0068,  0.0334,  0.0038, -0.0358,  0.0442,  0.0139, -0.0124,  0.0131,\n",
       "           0.0172,  0.0424,  0.0164,  0.0043, -0.0233,  0.0095, -0.0157, -0.0381,\n",
       "           0.0123, -0.0394, -0.0011,  0.0230,  0.0175,  0.0282,  0.0396,  0.0147,\n",
       "           0.0221, -0.0119,  0.0310, -0.0016, -0.0071, -0.0366,  0.0395,  0.0060],\n",
       "         requires_grad=True)),\n",
       " ('proj_Q.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0420,  0.0234,  0.0437,  ...,  0.0019, -0.0114,  0.0271],\n",
       "          [ 0.0254,  0.0102,  0.0208,  ..., -0.0207, -0.0187, -0.0138],\n",
       "          [-0.0020, -0.0079, -0.0201,  ...,  0.0061,  0.0411,  0.0153],\n",
       "          ...,\n",
       "          [ 0.0149,  0.0427, -0.0384,  ...,  0.0174, -0.0362,  0.0362],\n",
       "          [-0.0341, -0.0147, -0.0210,  ...,  0.0085, -0.0158,  0.0393],\n",
       "          [-0.0151, -0.0128,  0.0204,  ..., -0.0172,  0.0088,  0.0068]],\n",
       "         requires_grad=True)),\n",
       " ('proj_Q.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 3.9744e-02,  2.9484e-02,  2.2388e-02,  3.5231e-02, -4.1109e-02,\n",
       "           3.9153e-02,  4.5512e-04,  1.0846e-02,  4.4858e-03,  1.3122e-02,\n",
       "          -1.0234e-02,  4.3325e-02,  1.2827e-02, -7.4592e-03, -3.9810e-02,\n",
       "          -2.8174e-02, -2.3186e-02,  1.3246e-03,  2.9205e-02,  2.8248e-02,\n",
       "          -9.1072e-04,  1.2002e-02,  1.0201e-02, -2.2091e-02,  2.6716e-02,\n",
       "           4.2820e-02, -3.5961e-02,  3.4385e-02,  3.1530e-04,  1.2198e-02,\n",
       "          -3.5563e-02, -1.9839e-02,  2.5644e-02,  3.0781e-02,  1.2566e-02,\n",
       "           2.8161e-02,  8.6008e-03,  3.2880e-02,  2.8985e-02,  2.3827e-02,\n",
       "           7.0737e-03,  2.3733e-02,  3.2994e-02, -2.2464e-02, -3.1475e-03,\n",
       "          -4.0053e-02, -1.0073e-02,  1.6754e-02, -1.5874e-02,  1.6504e-02,\n",
       "          -4.1638e-02, -1.5437e-02, -2.0527e-02, -3.9072e-02,  4.1061e-02,\n",
       "           1.1550e-02,  1.7509e-02,  2.8832e-03, -1.0567e-02,  3.7859e-03,\n",
       "          -1.7552e-02, -2.7123e-02, -3.7224e-02, -2.2009e-02, -7.9432e-03,\n",
       "          -4.2886e-02, -1.6118e-03,  1.5741e-03,  2.7850e-02,  4.0375e-03,\n",
       "          -4.1129e-02,  3.2298e-02, -3.1219e-02,  3.2807e-02, -1.8370e-02,\n",
       "           2.8048e-02,  2.1022e-02, -2.9137e-02,  4.0324e-02, -3.5196e-02,\n",
       "           2.4357e-02,  3.8322e-02, -1.7987e-02,  1.9854e-02,  4.0023e-02,\n",
       "           4.0685e-02,  1.0205e-03,  1.2685e-02,  3.6168e-02, -3.7421e-02,\n",
       "           1.9261e-02,  4.1027e-02, -3.2647e-02, -2.7003e-02,  4.4182e-02,\n",
       "          -2.9103e-02,  1.0662e-02, -7.8842e-03,  1.5059e-02,  4.6798e-03,\n",
       "           1.5884e-02,  3.9730e-02, -2.4622e-02,  2.5508e-02, -4.4177e-02,\n",
       "          -2.1730e-02,  1.8722e-02,  4.9055e-03, -1.7456e-02, -1.2977e-03,\n",
       "           1.0568e-02, -1.4491e-02, -2.2191e-03, -2.4995e-02, -3.1786e-03,\n",
       "          -3.1825e-02,  2.9120e-02,  9.7440e-03,  1.1282e-02, -4.1546e-03,\n",
       "           2.9204e-02, -2.8354e-02,  2.9264e-02,  3.9476e-02,  2.9901e-03,\n",
       "          -4.0275e-02,  3.0147e-02,  2.4146e-02,  3.7688e-02,  9.4311e-03,\n",
       "          -1.5599e-02, -3.3438e-02,  2.6157e-02, -3.3875e-02, -5.9441e-03,\n",
       "          -1.6525e-02,  7.0978e-03,  4.0977e-02,  2.6173e-02, -2.7236e-02,\n",
       "          -3.1256e-02,  2.4221e-02, -2.5879e-02,  8.3634e-03,  2.3783e-02,\n",
       "           1.2019e-02,  2.8352e-02,  3.9535e-02,  2.3866e-02, -3.6486e-02,\n",
       "           1.9403e-02, -2.1418e-02,  3.7389e-03, -4.0913e-02, -3.6672e-02,\n",
       "          -3.6949e-02, -1.8638e-02, -4.0270e-02, -2.2979e-02,  5.3368e-03,\n",
       "           1.8538e-02,  3.4765e-02, -4.0420e-02,  4.3266e-02, -3.1931e-02,\n",
       "          -3.0206e-03,  2.0156e-02, -3.7058e-02,  2.9397e-03, -2.2041e-02,\n",
       "          -2.2807e-02,  1.7139e-02, -4.7047e-03, -4.4123e-02,  4.3316e-02,\n",
       "           2.7536e-02,  3.3533e-02, -2.1679e-02,  2.1031e-02, -2.6393e-02,\n",
       "           1.9842e-02, -3.2915e-02, -1.6978e-03,  1.9273e-02,  3.1165e-02,\n",
       "          -3.4609e-02,  2.6022e-02, -8.7618e-04, -2.3517e-02, -3.0412e-02,\n",
       "           1.4400e-02,  1.5281e-02,  1.9438e-02,  4.3790e-02,  3.1516e-02,\n",
       "           8.3776e-03,  3.8808e-02, -3.6770e-02,  1.0823e-02, -1.1323e-02,\n",
       "          -3.6419e-02, -1.8105e-02,  1.9577e-02,  3.3693e-02, -8.8380e-03,\n",
       "           7.7671e-05, -7.7568e-03, -2.4422e-02, -2.2835e-02, -4.2832e-02,\n",
       "          -1.7349e-02,  1.0271e-02, -3.5082e-02, -1.4690e-02, -1.0402e-02,\n",
       "           1.6218e-02,  3.7154e-02,  2.1751e-02, -2.4045e-02,  3.7331e-02,\n",
       "          -3.2743e-02,  3.4283e-02,  5.3364e-03,  3.5313e-02, -2.6878e-02,\n",
       "          -3.0106e-02,  3.4922e-02,  9.4261e-03, -1.6329e-02,  2.7059e-02,\n",
       "          -2.3834e-02,  3.3345e-03,  2.6521e-02, -3.2392e-02, -1.1781e-02,\n",
       "           6.1144e-03, -2.7796e-02, -1.4522e-02,  4.9637e-03,  3.9612e-02,\n",
       "          -3.0613e-02, -2.1084e-02, -2.6050e-02, -3.4254e-04,  4.0151e-02,\n",
       "           2.5080e-02,  8.6600e-03,  4.0338e-02, -2.1925e-02,  4.0399e-02,\n",
       "          -1.4134e-02,  1.0877e-02, -3.0398e-02,  1.6270e-02,  1.7663e-02,\n",
       "          -2.8639e-02, -3.8678e-02, -2.7828e-02, -1.2560e-02, -1.2767e-03,\n",
       "           7.6518e-03, -2.9187e-02, -3.5013e-02, -2.6562e-02, -3.2186e-02,\n",
       "          -3.6162e-02, -3.8830e-02, -1.8370e-02, -1.2566e-02,  3.1525e-02,\n",
       "           5.9005e-03,  4.3949e-02,  8.9782e-03,  2.9949e-03,  2.7873e-02,\n",
       "          -1.2731e-02,  6.8502e-03, -2.8596e-03,  8.9143e-03,  1.5589e-02,\n",
       "           3.0780e-02, -3.7698e-02, -3.2869e-02,  3.5677e-02,  4.9810e-03,\n",
       "          -3.5037e-02,  7.5901e-03, -6.1320e-03, -3.6829e-02, -4.0632e-02,\n",
       "           2.1408e-02,  3.8480e-02, -1.2488e-02,  7.2434e-03, -2.1084e-02,\n",
       "           2.7883e-02,  1.0650e-02, -1.1942e-02, -1.5207e-02, -2.7554e-02,\n",
       "          -8.5957e-03,  7.0786e-03, -1.5659e-02,  4.3620e-02,  2.6012e-02,\n",
       "           1.3782e-02,  3.3982e-02, -3.1075e-02, -2.6744e-02,  3.5819e-02,\n",
       "           3.9585e-02, -3.8306e-02,  2.6067e-02, -1.9688e-02,  2.2180e-02,\n",
       "           2.3421e-02, -3.4016e-02, -2.6559e-02, -2.5073e-03, -1.3448e-02,\n",
       "          -3.4962e-02,  2.1887e-02, -1.1393e-02, -4.1916e-03, -2.4504e-02,\n",
       "          -3.7103e-02,  3.7652e-02, -3.1784e-02, -1.3002e-02, -1.6637e-02,\n",
       "          -3.5600e-02, -9.1475e-03, -4.0386e-02,  1.8681e-02,  3.8570e-02,\n",
       "          -3.5698e-02,  8.8321e-03,  1.3143e-02, -9.9528e-03, -4.8910e-03,\n",
       "          -2.6132e-02, -1.3149e-02, -3.3087e-02,  1.0707e-03,  3.6062e-02,\n",
       "           4.4139e-02,  2.2559e-02, -1.3091e-02,  2.9234e-02,  2.3078e-02,\n",
       "           3.7704e-02, -2.3578e-02, -1.8347e-02, -2.5591e-02,  3.1151e-03,\n",
       "           9.7825e-03,  1.4013e-02,  3.2192e-02, -3.5135e-02, -3.5103e-02,\n",
       "           4.1992e-02, -1.5021e-02,  2.0682e-02,  3.2992e-02, -1.9559e-02,\n",
       "           4.2374e-02,  1.2634e-02,  3.5987e-02, -3.1250e-02, -3.0242e-02,\n",
       "          -5.1459e-03, -2.0062e-03, -9.1219e-03,  2.9615e-02,  3.8537e-02,\n",
       "           3.7821e-02, -3.2363e-02,  2.2257e-02,  4.4830e-03,  8.3653e-03,\n",
       "          -2.5550e-02, -4.4019e-02,  2.8890e-02, -2.8810e-02, -3.7208e-02,\n",
       "           9.5563e-03, -3.3238e-02,  3.6304e-02, -3.5337e-02,  4.0530e-02,\n",
       "          -5.4038e-03, -3.2487e-02,  2.4850e-02,  5.5444e-03, -3.0224e-02,\n",
       "           2.8312e-02, -3.5846e-02,  2.3316e-02, -2.1873e-02,  1.2369e-02,\n",
       "           2.1971e-02, -2.3374e-02,  3.1422e-02, -2.0102e-02,  3.1170e-02,\n",
       "          -1.9485e-02,  9.1749e-03, -2.8490e-02,  4.3781e-02,  4.2830e-02,\n",
       "           3.0747e-02, -1.2037e-02, -1.9788e-02, -3.4226e-02,  2.2443e-02,\n",
       "          -3.4888e-02, -9.3811e-03, -2.1025e-02,  1.7369e-02, -3.4475e-02,\n",
       "           1.5381e-03,  1.8374e-02,  2.2066e-02,  1.7887e-02,  1.7537e-02,\n",
       "          -4.3160e-02,  2.4085e-02, -7.5900e-03, -1.5509e-02, -2.8424e-03,\n",
       "          -1.5200e-02, -1.9188e-03, -9.6825e-03, -3.2558e-02,  4.3200e-02,\n",
       "           2.5501e-02,  3.2910e-03,  3.6514e-02,  4.3919e-04,  3.3912e-02,\n",
       "          -1.8419e-02,  4.1144e-02,  7.6985e-04,  2.2160e-02, -1.4577e-02,\n",
       "           9.3188e-03,  4.0869e-02,  2.4307e-02, -2.8668e-02, -2.1350e-02,\n",
       "          -4.8991e-03,  4.0212e-02, -6.8063e-03, -6.2441e-03,  1.9956e-02,\n",
       "           2.1927e-02,  1.7652e-02, -2.3056e-02, -4.8837e-03,  4.4038e-02,\n",
       "           2.3021e-02,  4.3818e-02,  4.1839e-02, -3.9982e-02,  3.4120e-02,\n",
       "          -3.5870e-02, -3.2061e-02, -2.6863e-02,  3.0827e-02,  1.0447e-02,\n",
       "           1.7663e-02, -7.6154e-04, -1.4269e-02,  3.0266e-02, -1.3914e-02,\n",
       "           3.5712e-02,  3.2533e-02,  1.1980e-02, -3.0852e-02, -3.9514e-03,\n",
       "           2.3178e-02,  3.1527e-02, -1.9849e-02, -3.0201e-02, -2.9838e-02,\n",
       "           1.6351e-03, -3.8305e-02,  3.3871e-02,  1.6740e-03,  2.6809e-02,\n",
       "           1.4515e-02, -2.9540e-02,  1.9208e-02,  7.9867e-04, -4.9140e-03,\n",
       "           3.6178e-03, -1.5222e-02,  2.6612e-02, -7.4491e-03,  2.3167e-02,\n",
       "           9.0109e-04, -4.0110e-02,  2.3062e-02,  3.0428e-02, -2.4691e-02,\n",
       "          -2.7864e-02, -3.5969e-02,  3.9624e-02, -5.8776e-03, -6.8909e-03,\n",
       "           2.4542e-02, -4.2182e-02], requires_grad=True)),\n",
       " ('proj_V.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-8.2716e-03, -1.7074e-02, -2.8231e-02,  ...,  1.3380e-02,\n",
       "           -1.4953e-02, -3.9003e-02],\n",
       "          [-1.4878e-02, -7.1229e-03,  2.6794e-02,  ..., -2.4061e-02,\n",
       "           -3.2127e-02, -1.3047e-02],\n",
       "          [ 7.8262e-03,  1.4878e-02, -4.2798e-02,  ...,  9.9857e-03,\n",
       "            1.7615e-02, -3.4935e-02],\n",
       "          ...,\n",
       "          [-6.9173e-03, -2.8045e-02, -2.0218e-02,  ..., -2.2730e-02,\n",
       "           -4.0612e-02, -3.8364e-02],\n",
       "          [ 1.3676e-02, -2.3402e-02,  3.4913e-02,  ...,  3.2820e-02,\n",
       "           -1.0667e-02,  8.6702e-03],\n",
       "          [-4.3201e-07, -4.6725e-04,  1.2307e-02,  ..., -3.1100e-02,\n",
       "            1.1348e-02,  9.5074e-03]], requires_grad=True)),\n",
       " ('proj_V.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.6215e-02,  1.9372e-02,  1.9039e-02,  1.6452e-02,  1.9621e-02,\n",
       "          -1.9532e-02, -2.7040e-02,  2.5612e-03, -1.2882e-02, -1.8485e-02,\n",
       "           6.2490e-03,  3.3802e-02, -3.2598e-02,  4.1429e-02, -3.3578e-02,\n",
       "          -3.5240e-02,  1.2068e-02,  3.0778e-02,  3.7934e-03, -3.0790e-02,\n",
       "          -4.8682e-03, -2.0549e-02, -2.1540e-02, -4.0235e-02,  1.2653e-02,\n",
       "          -3.4539e-02,  9.6473e-03,  2.0360e-02, -6.0263e-03, -3.1474e-03,\n",
       "           2.6685e-02, -2.6793e-02,  3.6628e-02,  4.0263e-02,  3.8876e-02,\n",
       "          -2.0763e-02, -1.8769e-02, -1.6811e-03, -2.4141e-02,  9.9883e-03,\n",
       "          -2.5855e-02, -2.0439e-03, -3.1907e-02, -4.2803e-02,  1.0116e-02,\n",
       "           7.4339e-03, -1.1919e-02, -7.0952e-03,  2.7212e-02,  4.4970e-03,\n",
       "          -4.3301e-02,  4.0449e-02,  3.9548e-02,  3.5847e-02, -3.9845e-02,\n",
       "          -4.3669e-02, -3.8740e-02,  1.9715e-02,  1.5271e-03, -1.2886e-02,\n",
       "           2.0830e-02, -3.7913e-02,  4.3500e-03,  2.9645e-02, -2.4453e-02,\n",
       "          -4.3900e-02, -2.6413e-02, -2.0547e-02,  8.5465e-03,  5.6419e-03,\n",
       "           3.1542e-02, -3.5535e-02,  2.2949e-02, -2.0307e-03, -1.4829e-02,\n",
       "          -3.9147e-02,  4.1379e-02, -2.1934e-02, -4.1025e-02,  3.7001e-02,\n",
       "           3.2415e-02, -2.9095e-02, -9.5203e-03,  2.3563e-02, -2.2717e-02,\n",
       "          -1.6347e-02,  3.1746e-02,  4.4571e-03,  1.1558e-02, -4.3726e-02,\n",
       "           1.1915e-02, -1.2341e-02,  4.3828e-02,  3.9429e-02, -3.6043e-02,\n",
       "          -2.5814e-02, -2.8340e-02, -1.3871e-02,  2.3588e-02,  3.7105e-02,\n",
       "           1.3311e-02,  2.3541e-02,  3.1525e-02, -1.2519e-02,  2.8703e-02,\n",
       "           2.2096e-02,  2.5698e-02, -4.0183e-02, -1.8023e-02, -6.8999e-04,\n",
       "           1.9335e-02,  3.1954e-02,  3.7546e-02, -2.1267e-02, -3.3709e-02,\n",
       "          -3.6623e-03, -9.2663e-03,  2.4245e-02,  1.2157e-03, -1.5945e-02,\n",
       "           1.8670e-02, -3.8130e-02,  1.2650e-02,  2.4519e-04, -2.4215e-02,\n",
       "          -4.5889e-03,  4.6095e-03,  1.0636e-02, -4.3594e-02, -1.3753e-02,\n",
       "          -5.9673e-03, -2.5269e-02,  4.0406e-02, -5.7032e-03, -2.6353e-02,\n",
       "          -1.2801e-02, -2.7471e-02, -5.7223e-03,  2.7672e-02, -2.1556e-02,\n",
       "          -1.8836e-02,  3.7791e-02, -8.7156e-03,  3.9956e-02,  2.6186e-02,\n",
       "          -1.9607e-02,  8.3421e-03, -1.9040e-03,  1.1660e-02,  3.7481e-02,\n",
       "           1.0568e-02, -5.7499e-03,  1.8551e-02, -2.1619e-02, -2.3386e-02,\n",
       "          -2.3062e-02,  4.2647e-02, -3.9019e-02,  2.2278e-02, -2.0957e-02,\n",
       "          -4.3139e-02, -4.2543e-02,  2.1360e-03,  3.6208e-02, -2.3374e-02,\n",
       "           2.8095e-02, -1.9283e-02,  4.1469e-02,  1.0738e-02,  6.0806e-04,\n",
       "          -2.3926e-02,  2.0421e-02, -4.1794e-02,  1.4469e-02,  3.2272e-02,\n",
       "           2.7473e-02,  4.4020e-02,  2.9323e-02,  1.3059e-02,  2.0680e-03,\n",
       "          -2.7181e-03,  2.4389e-03,  2.6537e-03, -8.5575e-03,  1.1613e-03,\n",
       "           2.7357e-02,  6.7504e-03, -2.2406e-02, -1.4702e-02,  1.1772e-02,\n",
       "           2.2016e-02,  1.2915e-02,  1.7657e-02, -3.0600e-02, -4.2000e-02,\n",
       "           1.1096e-02, -1.1397e-02,  5.3625e-03, -1.3802e-02,  1.1443e-02,\n",
       "           3.8701e-02, -2.8745e-03,  5.8279e-03,  7.5206e-03, -7.1024e-03,\n",
       "           2.5388e-02,  3.3285e-03, -1.0311e-02, -1.1350e-02, -1.9998e-02,\n",
       "           4.0079e-02, -2.0302e-02,  3.9029e-02,  8.5009e-03,  3.4229e-02,\n",
       "           1.1549e-02,  2.7509e-02,  9.6828e-03,  3.2962e-02,  1.6414e-02,\n",
       "           3.8291e-02,  2.9762e-02,  1.3390e-02, -3.5073e-02,  2.1995e-02,\n",
       "           5.1363e-03,  1.7943e-02,  2.5373e-02, -4.0504e-02, -2.4055e-02,\n",
       "           3.6550e-02, -8.8055e-03,  9.9714e-03, -3.4187e-03, -3.3057e-02,\n",
       "           3.4215e-02, -9.5568e-06,  3.8545e-02,  2.1096e-02,  2.6453e-02,\n",
       "          -4.1611e-02, -2.2114e-02,  1.2424e-02,  3.3543e-02, -1.4704e-02,\n",
       "           3.0273e-02, -2.3175e-02, -1.1861e-02,  2.8771e-02, -7.1954e-03,\n",
       "          -1.2229e-02,  2.0359e-02, -1.6818e-02, -1.9269e-02,  2.2941e-02,\n",
       "          -2.1357e-02,  3.6254e-03,  2.3217e-02, -3.3923e-02,  2.0444e-02,\n",
       "           3.1854e-02, -1.6794e-02, -2.0855e-02, -3.3399e-02, -9.1546e-03,\n",
       "          -2.2745e-02,  2.4182e-02,  2.4334e-03,  3.3660e-03,  4.0792e-02,\n",
       "          -1.6103e-02,  1.4462e-03,  4.2071e-02,  3.1284e-02,  2.0720e-02,\n",
       "          -1.5733e-02, -1.2865e-02,  6.8160e-04,  3.2710e-02, -4.2948e-02,\n",
       "           3.0893e-02,  1.7531e-02,  1.1443e-02, -4.6607e-03, -1.1159e-02,\n",
       "           2.1793e-02, -3.1149e-02, -1.6744e-02,  5.6998e-03, -1.7985e-02,\n",
       "           1.9953e-02,  1.9620e-02, -3.0185e-02,  4.2969e-02,  3.1354e-02,\n",
       "          -2.8556e-02, -3.6588e-03,  1.3710e-02,  2.6127e-02, -3.4079e-02,\n",
       "           3.0778e-02,  3.1710e-02, -6.9192e-03, -2.3789e-02,  2.3810e-02,\n",
       "           4.9165e-03, -2.5753e-02, -1.3464e-02,  3.2212e-02, -1.2749e-03,\n",
       "           2.1286e-02,  3.7365e-03,  9.8930e-04,  3.3421e-02, -1.4781e-02,\n",
       "          -2.9421e-02, -3.9652e-02, -3.5761e-02,  1.8593e-02, -6.3109e-03,\n",
       "          -2.7229e-02, -5.2822e-04, -3.7364e-02,  5.7039e-03, -3.9276e-02,\n",
       "          -3.1621e-03, -7.4723e-03, -6.6901e-03,  3.7709e-02, -3.5674e-02,\n",
       "           3.6657e-02, -3.2849e-02,  1.1267e-02,  2.8362e-03,  1.2122e-02,\n",
       "          -4.1274e-04,  2.5476e-03,  7.8223e-03,  2.5342e-02, -1.9297e-02,\n",
       "           2.0940e-02, -2.4907e-02,  3.6034e-02,  3.9952e-02, -1.7430e-03,\n",
       "          -3.6183e-02, -4.3203e-02, -4.1015e-02, -4.2965e-02,  6.8082e-03,\n",
       "           1.6412e-02, -2.3818e-02,  4.2742e-02, -1.4035e-02,  4.1640e-02,\n",
       "          -8.4283e-03,  4.3014e-02,  6.1682e-03, -3.8179e-02,  3.6347e-02,\n",
       "           3.4885e-02, -3.4386e-02,  1.6223e-02,  1.2774e-03, -4.1451e-02,\n",
       "          -8.8898e-03,  4.2255e-02,  2.5439e-02, -8.2672e-03, -2.9695e-02,\n",
       "           3.8751e-02, -3.7167e-03, -4.3591e-02,  2.2798e-02,  5.3220e-03,\n",
       "          -8.7759e-03, -1.9943e-03, -1.2517e-02, -1.5317e-02,  3.9216e-03,\n",
       "           4.1323e-02,  4.3865e-02, -3.8148e-02,  1.0138e-02, -5.5732e-03,\n",
       "          -4.3799e-02,  3.2958e-02, -5.1797e-04,  3.8689e-02, -3.8343e-02,\n",
       "          -3.2047e-02,  3.3589e-02, -2.2189e-02,  3.8225e-02,  1.5213e-02,\n",
       "          -3.4662e-02,  2.6652e-02,  2.5201e-02, -2.5317e-03,  2.9750e-02,\n",
       "          -2.8199e-02, -3.1197e-02, -2.7522e-03, -3.3818e-03,  4.3782e-02,\n",
       "           1.3935e-02,  2.6072e-02,  3.4523e-02, -3.2544e-02,  3.8724e-02,\n",
       "          -1.0671e-02,  3.8733e-02, -3.9754e-02, -1.6923e-02, -2.1604e-02,\n",
       "          -2.0496e-02, -2.2802e-02,  1.2520e-02,  2.6041e-02, -9.1301e-03,\n",
       "          -2.2373e-03,  8.9265e-03, -2.7138e-02, -3.8839e-02, -8.7074e-03,\n",
       "           1.0303e-02, -4.2917e-02,  2.9671e-02, -3.6889e-02,  2.1196e-03,\n",
       "          -3.1199e-02,  2.9644e-02,  3.9266e-03, -4.0478e-02,  3.1726e-02,\n",
       "          -2.0878e-02,  3.4304e-02,  1.7372e-02, -1.4270e-02, -3.6922e-02,\n",
       "           3.3583e-02, -2.9839e-02, -3.4822e-02,  3.5474e-02, -1.0357e-02,\n",
       "           4.3065e-02, -2.4074e-02,  7.3830e-03,  4.1184e-02,  4.0716e-02,\n",
       "          -1.9065e-02, -2.3172e-02,  1.5207e-02, -1.0702e-02, -2.3363e-02,\n",
       "           3.4719e-02, -4.2709e-02,  1.5089e-02,  2.4780e-02, -1.4708e-02,\n",
       "          -7.3434e-03, -3.1212e-02, -4.1445e-02,  3.0122e-04, -4.1060e-02,\n",
       "          -3.1623e-02,  2.9307e-02, -1.4562e-02, -2.2492e-02,  1.5392e-02,\n",
       "           1.2713e-02,  3.8393e-02,  1.6418e-02,  9.6336e-03,  2.5484e-02,\n",
       "          -1.5883e-02, -1.2690e-02,  4.3342e-02, -1.6936e-02,  1.1631e-02,\n",
       "           4.1984e-02,  3.9410e-02,  2.7600e-02,  4.1854e-02,  3.2060e-03,\n",
       "           1.1115e-02, -1.0889e-03, -3.1127e-02, -1.7718e-02, -2.7069e-02,\n",
       "           1.1116e-02,  2.0797e-02, -2.9709e-02,  1.1341e-02,  6.9519e-03,\n",
       "          -3.5961e-02,  2.3430e-02, -1.0471e-02, -2.9991e-02, -3.6748e-03,\n",
       "           4.2679e-03,  3.5808e-02,  2.7399e-02,  1.9804e-02,  2.9539e-02,\n",
       "          -3.8094e-02, -1.7522e-02,  3.1759e-02,  8.8171e-03,  2.7022e-02,\n",
       "          -4.0559e-02, -3.9347e-02], requires_grad=True)),\n",
       " ('proj_0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0414,  0.0065,  0.0065,  ..., -0.0354,  0.0440, -0.0126],\n",
       "          [ 0.0289, -0.0180, -0.0345,  ...,  0.0023,  0.0425, -0.0372],\n",
       "          [-0.0375,  0.0441, -0.0338,  ...,  0.0251,  0.0345,  0.0116],\n",
       "          ...,\n",
       "          [-0.0384, -0.0180,  0.0187,  ..., -0.0225, -0.0096,  0.0171],\n",
       "          [ 0.0417,  0.0308,  0.0258,  ...,  0.0203, -0.0040,  0.0370],\n",
       "          [-0.0306, -0.0321,  0.0145,  ...,  0.0338,  0.0385, -0.0295]],\n",
       "         requires_grad=True)),\n",
       " ('proj_0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.2663e-02, -1.1816e-02,  4.2821e-02,  4.3883e-02,  3.3893e-02,\n",
       "          -2.2400e-02, -1.2854e-02,  2.1624e-02, -3.9245e-03, -7.1865e-03,\n",
       "          -1.0685e-02,  2.7257e-02, -2.1497e-02, -2.5082e-02, -4.1090e-02,\n",
       "          -1.5849e-02, -2.9316e-02, -2.4881e-02, -4.1184e-02, -2.9201e-02,\n",
       "          -3.7052e-02,  4.1511e-02, -3.1434e-02,  8.7549e-03, -3.9912e-02,\n",
       "           3.6773e-02,  2.4732e-02, -1.8102e-02,  2.4868e-02,  3.7575e-02,\n",
       "           3.1208e-02,  3.0048e-02, -4.1764e-02,  2.6203e-02,  1.8317e-02,\n",
       "          -2.3806e-03, -5.1450e-03, -1.2642e-02, -2.5933e-02, -1.4906e-02,\n",
       "          -4.8939e-03, -1.8169e-02,  1.6342e-02,  3.6846e-02, -1.5821e-04,\n",
       "          -3.7321e-02,  3.9022e-02,  1.0303e-02,  8.4212e-03,  1.5097e-02,\n",
       "           1.1419e-03,  1.5053e-02,  1.7505e-02,  9.7373e-03,  2.5114e-02,\n",
       "          -7.6255e-03,  2.6614e-02, -8.9022e-03, -4.2411e-02, -2.2874e-02,\n",
       "           3.4620e-02,  5.9933e-03, -2.6992e-02,  3.3386e-02, -1.8980e-02,\n",
       "          -1.9380e-02, -1.9935e-02, -1.7892e-02, -1.3867e-02,  3.2784e-02,\n",
       "           1.4493e-02,  2.0286e-02, -3.5591e-02, -1.7412e-02,  3.3627e-02,\n",
       "           3.1566e-02, -3.1502e-02,  4.3641e-02,  1.3029e-02, -1.3373e-02,\n",
       "          -1.8061e-02, -3.5114e-02,  3.9685e-02,  7.5327e-03, -4.0456e-02,\n",
       "          -3.0642e-02,  3.5566e-02,  3.0493e-02,  6.3527e-03, -7.6921e-03,\n",
       "           2.9220e-02, -3.0319e-02, -3.7612e-02, -2.7102e-02,  1.0717e-03,\n",
       "          -3.1125e-02,  1.2915e-02,  2.6317e-02, -3.9409e-02,  2.9749e-02,\n",
       "          -1.8963e-02, -3.0559e-02, -4.3549e-02, -1.3126e-02,  6.9520e-03,\n",
       "          -4.1135e-02, -1.8420e-02,  7.5565e-03,  1.3742e-02,  4.2918e-04,\n",
       "          -8.0149e-03,  5.2241e-03,  2.3020e-02, -6.0640e-04, -7.8227e-03,\n",
       "          -1.9487e-02, -8.2144e-03,  2.7566e-02, -1.1890e-03, -2.1853e-02,\n",
       "           4.1959e-02, -3.4933e-02,  3.6494e-02, -3.2735e-02,  7.4571e-04,\n",
       "           8.9457e-04,  7.8919e-03,  4.0698e-02, -2.5149e-02,  1.3440e-02,\n",
       "           3.4413e-03,  4.1497e-02,  1.5427e-02,  3.7809e-02,  9.4568e-03,\n",
       "           1.3754e-02, -2.0482e-02,  3.9062e-02,  2.1620e-02, -1.2258e-02,\n",
       "           2.8168e-03, -3.3277e-02, -1.8385e-03,  1.1707e-02, -2.0000e-02,\n",
       "           1.4547e-02, -5.8232e-03,  3.3365e-02, -1.7474e-03,  2.8265e-02,\n",
       "          -1.8832e-02,  2.6169e-02,  2.4939e-02,  3.0355e-02, -1.2537e-02,\n",
       "          -4.1656e-03, -4.2889e-02,  3.5079e-02, -3.9556e-02, -4.3637e-04,\n",
       "           3.3613e-02,  1.2139e-02, -4.0800e-02,  4.5867e-03,  2.9558e-02,\n",
       "           3.7809e-02,  1.4708e-02, -4.2830e-02,  1.1722e-02, -1.0533e-02,\n",
       "          -3.9979e-02,  3.0767e-02,  1.5227e-02, -2.6117e-02,  3.1771e-02,\n",
       "           2.1850e-03,  1.6226e-02,  2.5440e-02,  4.3963e-03,  3.1092e-02,\n",
       "          -1.2834e-02,  2.8999e-02,  2.9560e-02,  2.5771e-02, -3.7064e-02,\n",
       "           2.5711e-02, -2.4983e-04,  2.3045e-02, -2.4914e-02, -1.8515e-02,\n",
       "          -3.5222e-02,  4.2384e-02, -3.2200e-02, -7.7946e-03, -1.2849e-02,\n",
       "          -2.9850e-03, -8.3573e-03, -3.4169e-02,  1.6872e-02,  1.1684e-02,\n",
       "          -1.3919e-02,  7.5429e-03,  2.0185e-03, -1.2499e-02,  3.0905e-02,\n",
       "          -1.8644e-02, -3.5415e-02, -1.3898e-02,  2.6994e-04, -3.7671e-02,\n",
       "          -1.5313e-02,  3.4213e-02,  2.1232e-02,  1.0773e-02, -4.1643e-02,\n",
       "          -7.8377e-04, -1.7899e-02, -1.0071e-02,  8.8721e-03,  3.1572e-02,\n",
       "          -3.3688e-02,  1.4116e-02, -7.8419e-03, -9.3262e-03, -4.4140e-02,\n",
       "          -3.7269e-02, -1.3254e-02,  2.7436e-02, -8.5611e-03, -1.7088e-03,\n",
       "           3.2216e-02,  2.9755e-02,  4.2715e-02, -2.1021e-02, -1.9771e-02,\n",
       "           3.1984e-02, -2.1744e-02,  2.0557e-02, -1.8641e-03,  4.3946e-02,\n",
       "           3.2252e-02,  3.7332e-02, -5.4638e-03,  4.4065e-02,  4.3394e-02,\n",
       "          -2.2454e-02, -9.2046e-03,  2.5829e-02, -4.1212e-02, -5.7516e-03,\n",
       "           7.7654e-03,  3.3763e-02, -1.5791e-02, -5.4479e-03,  2.9077e-02,\n",
       "          -3.8717e-02, -2.5914e-02, -2.8204e-02,  1.1289e-02, -7.4284e-07,\n",
       "          -2.9985e-02, -2.9808e-02,  6.7244e-04, -3.0209e-02, -8.1245e-03,\n",
       "           1.3646e-02, -5.3627e-03, -2.4222e-02, -2.5940e-02, -3.9512e-02,\n",
       "          -4.3242e-02, -2.2833e-02, -4.1344e-02, -2.8906e-02,  8.5923e-03,\n",
       "          -1.9667e-02, -2.2203e-02, -3.0835e-02,  3.0032e-02, -2.1345e-02,\n",
       "           3.4272e-03,  2.4950e-02, -1.2254e-02, -3.4330e-02,  3.8057e-02,\n",
       "           4.2545e-02,  8.7687e-03,  6.6220e-03, -1.4362e-02,  9.2721e-03,\n",
       "          -1.5084e-02, -3.0570e-02,  4.3027e-02, -1.9544e-02,  1.0110e-02,\n",
       "          -8.1388e-03,  4.3528e-02, -3.8222e-02, -1.5675e-02,  3.4801e-02,\n",
       "           2.0595e-03,  3.9297e-02,  1.0888e-02,  4.0097e-02, -5.2941e-04,\n",
       "          -1.3147e-02, -4.3115e-02, -4.0561e-02, -3.7714e-04,  2.5883e-02,\n",
       "          -1.1004e-02, -3.9059e-03,  2.5929e-02,  1.2552e-02,  3.8200e-02,\n",
       "           3.7868e-02, -2.9565e-02,  2.4635e-02,  4.3709e-03,  2.3664e-02,\n",
       "           2.8340e-02,  4.0121e-02, -2.7503e-02,  1.1102e-02, -3.2042e-02,\n",
       "           2.8185e-02, -3.8244e-02,  2.1837e-02, -1.7307e-02, -1.6861e-02,\n",
       "           1.4981e-02,  1.8339e-02,  3.1486e-02,  4.1873e-03, -4.2313e-03,\n",
       "           4.3285e-02, -3.6111e-02,  1.6945e-02,  4.2810e-02,  1.6964e-02,\n",
       "          -3.1315e-02,  9.9844e-03,  2.3889e-02,  2.2304e-02, -1.0629e-03,\n",
       "           2.0892e-02,  1.8196e-02, -2.7466e-02, -2.6076e-02,  2.9673e-02,\n",
       "           1.7322e-02, -1.0421e-02,  5.6531e-03,  1.2498e-02,  5.0833e-03,\n",
       "          -3.8969e-02, -2.9782e-02, -1.9863e-02, -2.3577e-03, -2.8943e-02,\n",
       "           2.6254e-02, -1.1603e-02,  2.0484e-02, -3.7134e-02,  2.7894e-02,\n",
       "           7.8701e-03, -1.5888e-02, -3.9810e-02,  3.8206e-02,  2.1561e-02,\n",
       "           2.3086e-02,  5.0408e-03, -2.7730e-02, -3.5948e-02, -3.7804e-02,\n",
       "          -1.0838e-02,  1.2653e-02, -1.6038e-02, -2.0935e-02, -1.6140e-02,\n",
       "          -1.2230e-02, -3.2865e-02, -3.8231e-03,  1.3742e-02, -4.0676e-02,\n",
       "           2.9785e-02, -3.3336e-02, -1.8941e-02, -7.1866e-04,  2.5050e-02,\n",
       "           2.1478e-02, -2.9227e-02,  1.6323e-02,  2.7255e-02,  8.4858e-03,\n",
       "          -6.0188e-03,  7.1427e-04, -1.7660e-02, -1.2818e-02, -1.0672e-02,\n",
       "           8.9569e-03,  6.9635e-03, -1.9383e-03,  2.1468e-02,  1.8297e-03,\n",
       "           3.8084e-02,  2.3797e-02,  3.1413e-02,  1.1632e-02,  3.8976e-02,\n",
       "           1.6954e-03,  1.9095e-02,  8.8712e-03,  3.5657e-02,  4.4193e-02,\n",
       "           3.1874e-02, -3.4112e-02, -3.4555e-02, -3.9565e-02,  2.0040e-02,\n",
       "           3.5132e-03,  2.2739e-03, -1.1230e-02,  2.8106e-02, -4.3589e-02,\n",
       "          -3.3305e-02,  1.9468e-02, -2.1682e-02, -6.5685e-03,  3.5192e-02,\n",
       "          -1.7288e-02, -2.8919e-02, -1.2040e-02,  4.3938e-02, -2.2449e-02,\n",
       "          -3.4900e-02, -4.2043e-02, -3.9089e-02, -1.3837e-02,  3.0696e-02,\n",
       "          -4.3456e-02, -4.0624e-02, -1.9946e-02, -3.9444e-03, -3.0707e-02,\n",
       "          -3.1862e-02,  1.1802e-02,  1.8308e-02, -1.2211e-02, -1.0156e-02,\n",
       "          -1.6283e-02, -4.1252e-02,  3.8179e-02,  3.8727e-02,  5.4224e-03,\n",
       "           1.3929e-02,  3.0593e-02, -2.6728e-02,  4.3644e-02, -1.9834e-02,\n",
       "           9.8534e-03, -4.2035e-02, -1.6048e-02,  3.0714e-02, -2.5662e-02,\n",
       "           2.3083e-02, -4.2595e-02, -1.8302e-02, -2.4309e-02,  3.3596e-02,\n",
       "          -7.3851e-03,  2.5335e-02, -3.9267e-02, -7.0795e-03, -3.0722e-02,\n",
       "          -1.3108e-02,  3.8738e-02, -9.7212e-03, -5.3749e-03, -2.3183e-02,\n",
       "           3.8853e-02, -2.8276e-03,  2.9578e-03,  1.1538e-02, -2.4712e-02,\n",
       "           1.7228e-02, -1.7919e-03,  1.2844e-02, -4.1222e-02,  1.1467e-02,\n",
       "          -3.8765e-02, -3.0666e-02, -1.1205e-02, -3.8296e-03,  3.4291e-02,\n",
       "          -4.0842e-02, -2.4362e-02,  3.9925e-02,  3.6600e-03, -4.1870e-02,\n",
       "          -7.7960e-03,  2.5480e-02, -1.5604e-02, -2.4185e-04,  4.3261e-02,\n",
       "           4.2106e-03, -2.5167e-02, -2.6998e-02, -1.1246e-02, -2.9934e-03,\n",
       "          -3.6636e-02, -4.1497e-03], requires_grad=True))]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(attention.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9527e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, linear_size):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(hidden_size, linear_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(linear_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00bc3927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 768\n",
    "linear_size = 2048\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size, 5, hidden_size)\n",
    "ffn = PositionWiseFFN(hidden_size, linear_size)\n",
    "ffn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4dc2d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc9364b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(batch_size, 5, hidden_size)\n",
    "layer_norm = LayerNorm(hidden_size)\n",
    "layer_norm(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f4811fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = ScaledProductAttention(hidden_size)\n",
    "        # Setting the linear size to 4 times the hidden size is a common choice.\n",
    "        self.feed_forward = PositionWiseFFN(hidden_size, hidden_size * 4)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        sub_layer_1 = self.layer_norm(self.attention(X, X, X, mask=mask) + X)\n",
    "        sub_layer_2 = self.layer_norm(self.feed_forward(sub_layer_1) + sub_layer_1)\n",
    "        return sub_layer_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "16da1ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(batch_size, 5, hidden_size)\n",
    "encoder_block = EncoderBlock(hidden_size)\n",
    "encoder_block(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c898f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_blocks = torch.nn.ModuleList([EncoderBlock(hidden_size) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # concat the output from calling each of self.encoder_blocks on x.\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, mask=mask)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e679092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(batch_size, 5, hidden_size)\n",
    "encoder = Encoder(hidden_size, 6)\n",
    "encoder(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4dbf190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.self_layer_norm = LayerNorm(hidden_size)\n",
    "        self.self_attention = ScaledProductAttention(hidden_size)\n",
    "        self.middle_layer_norm = LayerNorm(hidden_size)\n",
    "        self.middle_attention = ScaledProductAttention(hidden_size)\n",
    "        self.feed_forward = PositionWiseFFN(hidden_size, hidden_size * 4)\n",
    "        self.ffn_layer_norm = LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None, memory_mask=None):\n",
    "        x = self.self_layer_norm(self.self_attention(x, x, x, mask=memory_mask) + x)\n",
    "        x = self.middle_layer_norm(self.middle_attention(x, encoder_output, encoder_output, mask=mask) + x)\n",
    "        x = self.ffn_layer_norm(self.feed_forward(x) + x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fcab95bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 768])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input sequence length is 5 and output sequence length is 7.\n",
    "x = torch.randn(batch_size, 7, hidden_size)\n",
    "encoder_output = torch.randn(batch_size, 5, hidden_size)\n",
    "decoder_block = DecoderBlock(hidden_size)\n",
    "decoder_block(x, encoder_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3b4f0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, hidden_size, num_layers):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.decoder_blocks = torch.nn.ModuleList([DecoderBlock(hidden_size) for _ in range(num_layers)])\n",
    "        \n",
    "        def forward(self, x, encoder_output, mask=None, memory_mask=None):\n",
    "            for decoder_block in self.decoder_blocks:\n",
    "                x = decoder_block(x, encoder_output, mask=mask, memory_mask=memory_mask)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da3684c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 768])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = torch.randn(batch_size, 5, hidden_size)\n",
    "encoder = Encoder(hidden_size, 6)\n",
    "encoder_output = encoder(input_seq)\n",
    "output_seq = torch.randn(batch_size, 7, hidden_size)\n",
    "decoder = Decoder(hidden_size, 6)\n",
    "decoder(output_seq, encoder_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecfa2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.embedding = torch.nn.Embedding(config['vocab_size'], self.hidden_size)\n",
    "        # TODO: implement sinoid positional embedding as written in the paper.\n",
    "        self.pos_embedding = torch.nn.Embedding(config['max_seq_len'], self.hidden_size)\n",
    "        self.max_seq_len = config['max_seq_len'] # This is also called block_size, I think.\n",
    "        self.padding_idx = config['padding_idx']\n",
    "\n",
    "        self.encoder = Encoder(self.hidden_size, self.num_layers)\n",
    "        self.decoder = Decoder(self.hidden_size, self.num_layers)\n",
    "        # This is actaully different from the embedding matrix.\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, config['vocab_size'])\n",
    "\n",
    "    def _gen_max_seq_mask(self, seq_matrix, row_length=None):\n",
    "        \"\"\"\n",
    "        seq_matrix: (batch_size, max_seq_len) with paddings at the end.\n",
    "        row_length: if this is not None, we will expand the row to this length.\n",
    "            if this is None, we will expand to max_seq_len.\n",
    "        returns: (batch_size, max_seq_len or row_length, max_seq_len) with 1s for the meaningful tokens\n",
    "                    and 0s for the paddings (at the end of each row).\n",
    "        \"\"\"\n",
    "        mask = seq_matrix != self.padding_idx\n",
    "        mask = mask.type(torch.float32)\n",
    "        row_length = row_length if row_length is not None else seq_matrix.shape[1]\n",
    "        return mask.unsqueeze(1).expand(-1, row_length, -1)\n",
    "\n",
    "    def _gen_autoregressive_mask(self, seq_matrix):\n",
    "        \"\"\"\n",
    "        seq_matrix: (batch_size, max_seq_len) with paddings at the end - similar to _gen_max_seq_mask.\n",
    "            this will mask out all the \"future\" tokens in the sequence.\n",
    "        returns: (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        max_seq_len = seq_matrix.shape[1]\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "        mask = mask.type(torch.float32)\n",
    "        return mask.unsqueeze(0).expand(seq_matrix.shape[0], -1, -1)\n",
    "\n",
    "    def forward(self, input_ids, memory=None):\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, max_seq_len). Each element is an integer representing the index of the token in the vocabulary.\n",
    "            max_seq_len is the maximum sequence length of the input in the batch.\n",
    "            all inputs are padded to this length for parallel processing.\n",
    "        output_ids: (batch_size, max_seq_len). This represents the output.\n",
    "        output dimension is (batch_size, output_seq_len, vocab_size).\n",
    "\n",
    "        NOTE: padding should be added to input_ids and output_ds (and specified by padding_idx).\n",
    "        TODO: who's doing the shifting?\n",
    "\n",
    "        a simple forward pass to either predict 1 token or use in training (compare against multiple IDs).\n",
    "        \"\"\"\n",
    "        assert input_ids.shape[1] <= self.max_seq_len, f\"input sequence length {input_ids.shape[1]} is greater than max sequence length {self.max_seq_len}!\"\n",
    "        assert memory.shape[1] <= self.max_seq_len, f\"output sequence length {memory.shape[1]} is greater than max sequence length {self.max_seq_len}!\"\n",
    "        input_mask = self._gen_max_seq_mask(input_ids)\n",
    "        encode_decode_att_mask = self._gen_max_seq_mask(input_ids, row_length=memory.shape[1])\n",
    "\n",
    "        memory_mask = self._gen_max_seq_mask(memory)\n",
    "        memory_mask = self._gen_autoregressive_mask(memory) * memory_mask\n",
    "\n",
    "        input_embeds = self.embedding(input_ids)\n",
    "        input_embeds += self.pos_embedding(torch.arange(input_embeds.shape[1])) # (batch_size, max_seq_len, hidden_size)\n",
    "        memory_embeds = self.embedding(memory) # (batch_size, max_seq_len, hidden_size)\n",
    "        memory_embeds += self.pos_embedding(torch.arange(memory_embeds.shape[1])) # (batch_size, max_seq_len, hidden_size)\n",
    "\n",
    "        encoder_output = self.encoder(input_embeds, mask=input_mask)\n",
    "        # NOTE: some implementation passes in the input_mask here. I think that is wrong.\n",
    "        decoder_output = self.decoder(memory_embeds, encoder_output, mask=encode_decode_att_mask, memory_mask=memory_mask)\n",
    "        linear_out = self.linear(decoder_output) # (batch_size, output_seq_len, vocab_size)\n",
    "        return torch.softmax(linear_out, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "19cc6599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer({'hidden_size': hidden_size, 'num_layers': 6, 'vocab_size': 1000, 'max_seq_len': 512, 'padding_idx': 0})\n",
    "input_ids = torch.randint(0, 1000, (batch_size, 5))\n",
    "input_ids[:, -2:] = 0\n",
    "memory_ids = torch.randint(0, 1000, (batch_size, 7))\n",
    "memory_ids[:, -3:] = 0\n",
    "model(input_ids, memory_ids).shape\n",
    "a = model._gen_max_seq_mask(memory_ids)\n",
    "b = model._gen_autoregressive_mask(memory_ids)\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb75218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
